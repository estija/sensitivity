{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyizhou/anaconda3/envs/math/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "import pdb\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from RoBERTa import *\n",
    "from LSTM import *\n",
    "from GPT2 import *\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import logging    # first of all import the module\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "parser.add_argument('--train_num_points', type=int,             default = 1000,            help='train data number')\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 500,               help='validation data number')\n",
    "parser.add_argument('--report_num_points',type=int,             default = 500,              help='report number')\n",
    "parser.add_argument('--model_name',       type=str,             default = 'roberta-scratch',   help='model name')\n",
    "parser.add_argument('--max_length',       type=int,             default=32,                 help='max_length')\n",
    "parser.add_argument('--num_labels',       type=int,             default=3,                 help='num_labels')\n",
    "parser.add_argument('--batch_size',       type=int,             default=4,                help='Batch size')\n",
    "parser.add_argument('--num_workers',      type=int,             default=0,                  help='num_workers')\n",
    "parser.add_argument('--replace_size',     type=int,             default=3,                  help='to test sensitivity, we need to replance each word by x random words from vocab, here we specify the x')\n",
    "parser.add_argument('--epochs',           type=int,             default=500,                  help='num of epochs')\n",
    "parser.add_argument('--lr',               type=float,           default=1e-4,              help='lr')\n",
    "parser.add_argument('--weight_decay',     type=float,           default=0.0,                 help='weight_decay')\n",
    "parser.add_argument('--gamma',            type=float,           default=1,                 help='lr*gamma after each test')\n",
    "parser.add_argument('--vocab_size',       type=int,             default=-1,                help='size of vocabulary')\n",
    "parser.add_argument('--embedding_dim',    type=int,             default=256,               help='embedding dimension')\n",
    "parser.add_argument('--hidden_dim',       type=int,             default=256,               help='hidden dimension')\n",
    "parser.add_argument('--num_head',         type=int,             default=4,               help='num of head in lstm')\n",
    "parser.add_argument('--num_layers',       type=int,             default=8,                 help='number of layers for LSTM and roberta')\n",
    "parser.add_argument('--dropout',          type=float,             default=0.2,               help='dropout')\n",
    "parser.add_argument('--pad_idx',          type=int,             default=1,                 help='ignores token with this index')\n",
    "parser.add_argument('--sensitivity_method',type=str,            default='word',                 help='embedding/word')\n",
    "parser.add_argument('--embedding_noise_variance',         type=float,            default=15,                 help='the variance of the noise for the embedding sensitivity testing')\n",
    "parser.add_argument('--exp_name',                          type=str,            default='default',                 help='why you run this experiment?')\n",
    "parser.add_argument('--dataset',                          type=str,            default='imdb',                 help='data name')\n",
    "parser.add_argument('--roberta_act',        type=str, default='softmax' , help='softmax/relu')\n",
    "parser.add_argument('--optimizer',        type=str, default='AdamW' , help='AdamW or SGD')\n",
    "parser.add_argument('--cudaname',        type=str, default='cuda:9' , help='whichgpu')\n",
    "args = parser.parse_args([])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "\n",
    "device = args.cudaname\n",
    "#foldername = datetime.now().strftime(f'./logs/{args.dataset}/{args.model_name}/%Y_%m_%d_%H_%M_%S')\n",
    "if 'roberta' in args.model_name and args.roberta_act == 'relu':\n",
    "    foldername = f'./logs/{args.dataset}/{args.model_name}_{args.roberta_act}/{args.exp_name}'\n",
    "else:\n",
    "    foldername = f'./logs/{args.dataset}/{args.model_name}/{args.exp_name}'\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "# Define the log filename inside the newly created folder\n",
    "logfilename = os.path.join(foldername, 'logfile.log')\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(filename=logfilename, filemode='w', format='%(asctime)s %(levelname)s - %(funcName)s: %(message)s')\n",
    "handle = \"root\"\n",
    "logger = logging.getLogger(handle)\n",
    "logger.info(f'args:{args}')\n",
    "\n",
    "\n",
    "if(args.dataset == 'boolq'):\n",
    "    dataset = load_dataset('boolq')\n",
    "elif (args.dataset in ['mnli', 'qqp', 'rte', 'qnli', 'mrpc']):\n",
    "    dataset = load_dataset('glue', args.dataset )\n",
    "elif(args.dataset == 'imdb'):\n",
    "    dataset = load_dataset('stanfordnlp/imdb')\n",
    "    contrast_data_validation = pd.read_csv('contrast_data_set/test_original.tsv', sep='\\t')\n",
    "    contrast_data_validation['Sentiment'] = contrast_data_validation['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "    contrast_origin_data = {\n",
    "        'label': contrast_data_validation['Sentiment'].tolist(),\n",
    "        'text': contrast_data_validation['Text'].tolist(),\n",
    "        'dataset':'imdb'\n",
    "    }\n",
    "    contrast_data_contrast = pd.read_csv('contrast_data_set/test_contrast.tsv', sep='\\t')\n",
    "    contrast_data_contrast['Sentiment'] = contrast_data_contrast['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "    contrast_contrast_data = {\n",
    "        'label': contrast_data_contrast['Sentiment'].tolist(),\n",
    "        'text': contrast_data_contrast['Text'].tolist(),\n",
    "        'dataset':'imdb'\n",
    "    }\n",
    "\n",
    "logger.info('\\n Property of dataset:')\n",
    "logger.info(f'train set size: {len(dataset[\"train\"])}')\n",
    "# logger.info(f'validation_mismatched set size: {len(dataset[\"validation_matched\"])}')\n",
    "# logger.info(f'test_matched set size: {len(dataset[\"test_matched\"])}')\n",
    "# logger.info(f'test_mismatched set size: {len(dataset[\"test_mismatched\"])}')\n",
    "# %%\n",
    "# %%\n",
    "\n",
    "train_num_points = min(args.train_num_points, len(dataset['train']))\n",
    "train = dataset['train'].shuffle(seed=42).select(range(train_num_points))\n",
    "\n",
    "if(args.dataset in ['boolq', 'qqp', 'rte', 'qnli', 'mrpc']):\n",
    "    valid_num_points = min(args.valid_num_points, len(dataset['validation']))\n",
    "    valid = dataset['validation'][-valid_num_points:]\n",
    "    valid['dataset'] = args.dataset\n",
    "    replaced = replaced_data_binary(valid, args.replace_size) \n",
    "    replaced['dataset'] = args.dataset\n",
    "elif (args.dataset == 'mnli'):\n",
    "    valid_num_points = min(args.valid_num_points, len(dataset['validation_matched']))\n",
    "    valid = dataset['validation_matched'][-args.valid_num_points:]\n",
    "    valid['dataset'] = args.dataset\n",
    "    replaced = replaced_data(valid, args.replace_size) \n",
    "    replaced['dataset'] = args.dataset\n",
    "elif (args.dataset == 'imdb'): #we just test the accuracy on contrast set, valid dataset now is the contrast origin.\n",
    "    valid_num_points = min(args.valid_num_points, len(dataset['test']))\n",
    "    shuffled_test = dataset['test'].shuffle(seed=42)\n",
    "    valid = shuffled_test[-args.valid_num_points:]#valid = contrast_origin_data\n",
    "    valid['dataset'] = args.dataset\n",
    "    replaced = replaced_data_binary(valid, args.replace_size) \n",
    "    replaced['dataset'] = args.dataset\n",
    "    contrastset = contrast_contrast_data\n",
    "\n",
    "# Shuffle the subset and then select the number of points for evaluation\n",
    "embedding_sens_eval_data = train.shuffle(seed=42).select(range(valid_num_points))[:]\n",
    "train = train[:]\n",
    "train['dataset'] = args.dataset\n",
    "embedding_sens_eval_data['dataset'] = args.dataset\n",
    "\n",
    "\n",
    "if args.model_name=='roberta-scratch' or args.model_name=='gpt2-scratch':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "elif args.model_name=='lstm':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "args.vocab_size = tokenizer.vocab_size\n",
    "#mnli\n",
    "#The Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. The authors of the benchmark use the standard test set, for which they obtained private labels from the RTE authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) section. They also uses and recommend the SNLI corpus as 550k examples of auxiliary training data.\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([None, 'text', 'sequence_length', 'dataset'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (490470676.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    embedding_sens_eval_data = get_Dataset_binary(embedding_sens_eval_data, tokenizer, args.model_name, max_length = args.max_length)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# pdb.set_trace()\n",
    "contrast_data = None\n",
    "if(args.dataset in ['boolq', 'rte', 'qqp', 'qnli', 'mrpc','imdb']):\n",
    "    train_data = get_Dataset_binary(train, tokenizer, args.model_name, max_length=args.max_length)\n",
    "    train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    valid_data = get_Dataset_binary(valid, tokenizer, args.model_name, max_length=args.max_length)\n",
    "    valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    if args.dataset!='imdb':\n",
    "        replaced_data = get_Replaced_Dataset_binary(replaced, tokenizer, args.model_name, max_length = args.max_length)\n",
    "        replaced_dataloader = DataLoader(replaced_data, sampler=SequentialSampler(replaced_data), \n",
    "                                batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    else:\n",
    "        replaced_dataloader = None\n",
    "    embedding_sens_eval_data = get_Dataset_binary(embedding_sens_eval_data, tokenizer, args.model_name, max_length = args.max_length)\n",
    "    embedding_sens_eval_dataloader = DataLoader(embedding_sens_eval_data, sampler=SequentialSampler(replaced_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    print(\"Length of Embedding Sensitivity Eval Loader\", len(embedding_sens_eval_dataloader))\n",
    "elif (args.dataset == 'mnli'):\n",
    "    train_data = get_Dataset(train, tokenizer, args.model_name, max_length=args.max_length)\n",
    "    train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    valid_data = get_Dataset(valid, tokenizer, args.model_name, max_length=args.max_length)\n",
    "    valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    replaced_data = get_Replaced_Dataset(replaced, tokenizer, args.model_name, max_length = args.max_length)\n",
    "    replaced_dataloader = DataLoader(replaced_data, sampler=SequentialSampler(replaced_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "    embedding_sens_eval_data = get_Dataset(embedding_sens_eval_data, tokenizer, args.model_name, max_length = args.max_length)\n",
    "    embedding_sens_eval_dataloader = DataLoader(embedding_sens_eval_data, sampler=SequentialSampler(replaced_data), \n",
    "                            batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "if args.dataset == 'imdb':                            \n",
    "    contrast_data = get_Dataset_binary(contrastset, tokenizer, args.model_name, max_length=args.max_length)\n",
    "    contrast_dataloader = DataLoader(contrast_data, sampler=SequentialSampler(contrast_data), \n",
    "                                batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "\n",
    "# %%\n",
    "if args.model_name=='roberta-scratch':\n",
    "    model = TextClassifier(args,foldername).to(device)\n",
    "elif args.model_name=='roberta-base':\n",
    "    model = TextClassifier(args,foldername).to(device)\n",
    "elif args.model_name=='gpt2-scratch':\n",
    "    model = GPT2TextClassifier(args,foldername).to(device)\n",
    "elif args.model_name=='lstm':\n",
    "    model = LSTMTextClassifier(args,foldername).to(device)\n",
    "model.train(train_dataloader,valid_dataloader,embedding_sens_eval_dataloader,replaced_dataloader,device,contrast_dataloader=contrast_dataloader)\n",
    "\n",
    "sizeinmb = get_model_size(model)\n",
    "logger.info(f'model size: {sizeinmb}MB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,   250, 32007, 49069,  3809,  1589, 49007,  3809, 48709,  2264,\n",
       "           130,  1617,    64,  6210,    47,    10, 14353,   822,   116,    83,\n",
       "          1563,  9850,     4,  1941, 34329,  2444, 10698,   167,  1617,  6683,\n",
       "             4,     2]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>A spoiler.<br /><br />What three words can guarantee you a terrific film? A Canadian Production. THE BRAIN fits those words perfectly.</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(contrast_data[0][2])\n",
    "tokenizer.decode(contrast_data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>Back in 1985 I caught this thing (I can't even call it a movie) on cable. I was in college and I was with a high</s>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[-50:][2])\n",
    "tokenizer.decode(train_data[17][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
