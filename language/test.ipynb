{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import logging    # first of all import the module\n",
    "from datetime import datetime\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "model =AutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels= 3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.shape)\n",
    "x.repeat(4,2,2).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_34020/1330882883.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kevin\\AppData\\Local\\Temp/ipykernel_34020/1330882883.py\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    for j in range(n):  # n different noises for each token\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import toch\n",
    "x_emb = torch.ones((3,4,5))\n",
    "repeated_matrices = x_emb[i].repeat(n*seqlen[i],1,1)       #TODO: add noise \n",
    "# #the dim of the embedding is max_len, embedding size\n",
    "# we first repeat the embedding n*seqlen[i] times\n",
    "# we generate a guassian matrix and add to it\n",
    "# do we add noise n times to each token emb?\n",
    "logger.debug(f\"repeated_matrices:{repeated_matrices}\") \n",
    "mean = 0\n",
    "variance = 15#https://arxiv.org/pdf/2211.12316v1.pdf set it to 15, is not it too large? as the embedding is range from -1 to 1 when initialization\n",
    "std_dev = variance ** 0.5\n",
    "guass =  torch.randn(n*seqlen[i],x_emb[i].shape[-1]) * std_dev + mean\n",
    "for k in range(seqlen[i]):\n",
    "    for j in range(n):  # n different noises for each token\n",
    "        noise = guass[k * n + j]\n",
    "        repeated_matrices[k * n + j, k, :] += noise.to(device)\n",
    "noised_embedding =repeated_matrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
